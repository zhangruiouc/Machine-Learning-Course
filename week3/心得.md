# Classification</br>
Classification类别的问题输出的结果一般是有限的数字或者类别。常见的为二元的分类问题。</br>
在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈。</br>
# Hypothesis Representation</br>
在如图所示的tumor prediction问题中，我们用到了逻辑回归模型。逻辑函数常用的为Sigmoid function，公式可以表示为：</br>
![image](https://user-images.githubusercontent.com/130215873/236690507-2323d8b5-7e68-4740-99b9-15680b39829f.png)</br>
函数的图像为：</br>
![1073efb17b0d053b4f9218d4393246cc](https://user-images.githubusercontent.com/130215873/236690479-399bffe1-4ded-41e4-840f-49279421e017.jpg)</br>
我们可以这样理解模型，输出的值在0到1之间。比如输出的值为0.8，则认为对于给定的输入变量，其结果为1的可能性为70%；反过来，其可能性为0的概率为30%。</br>
# Decision Boundary
决策边界可以理解为对数据集进行划分的一条线。在这条线的左边(内部)是一个类别，比如0；在这条线的右边(外部)是一个类别，比如1。对这条线的确定，其实就是对函数g(z)中z的处理得来的。</br>
![6590923ac94130a979a8ca1d911b68a3](https://user-images.githubusercontent.com/130215873/236690805-aab6c101-cb74-4ac5-a535-3085c14aebdc.png)</br>
比如对于图中的g(z)，对z做处理：</br>
* z=0时，g(z)=0.5
* z>0时，g(z)>0.5
* z<0时，g(z)<0.5</br>
可以看到，z=0就是一条分界线。把z<0的一边归为类别0，把z>0的一边归为类别1。同样的，对于各种别的情况，也可以对g(z)中的z处理，令z=0来找到边界。</br>
# Loss Function
对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义。但是如果照搬线性回归的定义，得到的代价函数是一个非凸的函数，这意味着我们的代价函数会有很多的局部最小值，这将影响梯度下降算法寻找全局的最小值。</br>
按照如图的定义来构造我们的代价函数：</br>
![image](https://user-images.githubusercontent.com/130215873/236692789-b043fc7a-1a3e-4f16-ad3c-8f9f0d8d8c6d.png)</br>
可以得到代价函数的图像如图所示：</br>
![ffa56adcc217800d71afdc3e0df88378](https://user-images.githubusercontent.com/130215873/236691151-296d44b2-0a3b-4a75-9019-720fbc27ed70.jpg)</br>
图中展示的代价函数分为两部分，以y=1的情况来分析代价函数。当预测的y为1时，代价函数非常接近x轴，说明此时预测的y与数据中的y误差非常小；当预测的y为0时，代价函数的曲线靠近y轴，且接近无穷大，说明此时预测的y与数据中的y误差非常大。</br>
# Simplified Cost
在逻辑回归中，Loss Function还可以写成更简单的形式：</br>
![image](https://user-images.githubusercontent.com/130215873/236691452-f961fe42-53e9-41ef-8e40-29b1f6e9ef05.png)</br>
这样的Loss Function可以同时包含y=1和y=0的两种情况。当y=1时，(1-y)为0；当y=0时，y=0。非常巧妙地分别将Loss Function中的一项式子变成0。</br>
# Gradient Descent
在逻辑回归的梯度下降中，同样用到了线性回归中的迭代方法。不同的是在逻辑回归中的函数表达式与线性回归的函数表达式并不相同。</br>
对于逻辑回归中的代价函数，迭代的参数中的w和b可以分别写成：</br>
![image](https://user-images.githubusercontent.com/130215873/236691861-0aaa531a-06b1-45e9-a415-2ba4bc9836de.png)</br>
# Overfitting
在课程中，我们将逻辑回归的曲线与data set的拟合程度分为三种类型，分别是underfit(high bias)，just right(generalization)和overfit(high variance)。</br>
![image](https://user-images.githubusercontent.com/130215873/236693957-e47bdd10-b2a3-4117-af7c-72e4b9b600b6.png)</br>
其中，过度拟合的情况和underfit的情况都不能很好地做预测。underfit的情况可能是因为我们考虑的特称太少了，而过度拟合的原因可能是因为我们考虑到的特征太多了，就像图中所示，不只考虑到了x，还有x^2，x^3等。</br>
# Reduce Overfitting
过拟合产生的原因有很多种。比如，当我们考虑到所有的特征，但是数据集太小时可能会产生过拟合的现象，此时我们可以选择继续收集数据。但有时数据的收集并不方便，此时可以采用generalization(正则化)的方法来解决过拟合。正则化的方法让我们对每个feature前的w的数值取何止做一个选择，将w变小，可以减少这个feature对预测结果的影响，同时又保留所有的feature。</br>
![image](https://user-images.githubusercontent.com/130215873/236694353-c993223d-ba0f-4c44-a2e7-b7c3ecc99f9c.png)</br>
如图所示，我们选择了更小的w，这可以让x^3和x^4对预测结果的影响更小，但又保留这些feature对预测结果的影响，并不消除它。</br>
# Rewrite Cost Function
在逻辑回归中，为了解决过拟合的问题，我们重写代价函数。这个代价函数引入λ，记作正则化参数。重写的代价函数中，加入了一项如图所示的regularization term。</br>
![image](https://user-images.githubusercontent.com/130215873/236694556-bf2b59b4-6dcd-4974-a3ae-a9f0a3448b87.png)</br>
可以看到，我们的代价函数包括两部分，分别是mean squared error和regularization term。mean squared error的作用是为了fit data，而regularization term的作用则是为了让特征值对应的w变小。它们两者的共同目标都是为了让代价函数尽可能地小，以此来找到代价函数的最小值对应的参数w和b</br>
当λ=0时，regularization term失去了意义，无法解决过拟合的问题；而当当λ太大时，为了让代价函数最小，此时一系列的参数w都会被设置地非常小，甚至接近0，这样最后曲线就会接近一条直线，那么我们的学习算法就遇到了underfit的问题。</br>
# How to update parameters between Linear Regression and Logistic Regression
对于线性回归和逻辑回归，我们按照如下所示的方法来对w和b更新。</br>
![image](https://user-images.githubusercontent.com/130215873/236694889-7aff6dc9-0cfd-4f49-8c50-6e70d4f46c3c.png)</br>
要注意到，此时线性回归和逻辑回归对应的函数表达式并不相同：</br>
![image](https://user-images.githubusercontent.com/130215873/236695215-eb9c7997-7329-4722-a770-e83de82200e9.png)
# 实验
本周的实验同样可以在jupyter上直接运行，在解决了matplotlib的版本兼容问题后，所有的可视化效果都可以正常显示了。这里我选了三个对我映像比较深的实验。
## 逻辑回归的代价函数
在本次的实验中，我们用用到了逻辑回归中的Loss Function。可视化的方法让我直观地感受到了为什么逻辑回归中直接使用Squared Error Cost并不能满足梯度下降迭代的需求。如图所示，分别展示了使用传统的Squared Error Cost和逻辑Loss Function时的情况。其中，只使用到了传统的Squared Error Cost时，函数图像并不利于梯度下降的方法来找到能使得代价最小的点。</br>
![image](https://user-images.githubusercontent.com/130215873/236695791-e36edc37-f27c-43ea-9019-da75f942a3d7.png)</br>
![image](https://user-images.githubusercontent.com/130215873/236695818-8059f2d2-77c3-409a-8503-4c75a847ff25.png)</br>
## 梯度下降
在逻辑回归中，梯度下降时采用的对w和b的更新和在线性回归中的方法是类似的，但是线性回归和逻辑回归对应的表达式不同。</br>
如图所示，在经历了9000次的迭代后我们认为代价函数已经收敛了，此时根据迭代后得到的w和b，可以对数据集中的对象做划分。如图所示，得到了较好的划分效果。</br>
![image](https://user-images.githubusercontent.com/130215873/236696028-d79213ab-cf65-4dc7-a6fc-fbb7032c0bfd.png)
## 过拟合
对于过拟合的现象，本节课的实验中很形象地用可视化来表现出来。如图所示的这种情况是我加了更多的数据点后拟合的结果，可以看出来出现了比较严重的过拟合现象。</br>
![image](https://user-images.githubusercontent.com/130215873/236696420-02ad5f95-022e-4f60-80a6-0337af3d2eec.png)</br>
过拟合的现象可以通过改变λ的值来改善。如图所示，将λ的值从0.02改为1后，代价函数的regularization term发挥了更大的作用，较好地处理了过拟合的情况。
![image](https://user-images.githubusercontent.com/130215873/236696350-5bee65da-855b-4eb4-85d3-162169361f88.png)















