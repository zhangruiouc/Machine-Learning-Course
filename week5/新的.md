# Tensorflow实现
图中所示的是一个Tensorflow网络的实现。网络的实现可以分为三部分：</br>
(1)利用dense创建每一层网络，同时用sequential将每一层网络连接。</br>
(2) 编译model，同时根据需要指定一个loss function。</br>
(3)利用fit来训练模型，epochs是梯度下降的递归次数。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bf69977a-5d3c-4d27-a1f9-62cf68fad6af)</br>
# 不同的激活函数
图中所示的是三种不同的activation函数。网络层中的每一层都可以选择一个激活函数。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/d6f0a383-75aa-4d69-9ee3-7e4dbf0d541b)</br>
三种激活函数分别是：</br>
(1)linear activation function，也可以理解为no activation function。函数值的范围可正可负。</br>
(2)sigmoid。函数值的范围从0到1。</br>
(3)ReLU。函数值大于等于0。</br>
# 如何选择激活函数
激活函数的选择对网络的性能有较大的影响。根据经验，在hidden layer中，往往选择ReLU作为激活函数。在output layer中选择激活函数可以按照如下的方法：</br>
(1)如果我们想解决的是一个binary classification问题，可以选择sigmoid。</br>
(2)如果我们想解决的问题中输出的是一个可正可负的数，选择linear activation。</br>
(3)如果我们想解决的问题的输出都是大于等于0的，那么我们可以选择ReLU。</br>
如图所示是一个简单的网络，在hidden layer中我们选择ReLU，而在output layer中我们选择sigmoid。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/92611eda-c576-4ae4-86df-9db60ffb28d0)</br>
需要注意的是，我们一般在hidden layer中不使用linear activation。举例说明，如果我们在每一层的hidden layer中都是用linear activation。那么我们的网络最后实现的功能仅仅与最后一层的output layer有关，如果我们的output layer选择sigmoid，那么整个网络所实现的功能仅仅只有logistic regression，前面那么多层的网络都在做无用功。</br>
# multiclass classification
如图所示为binary classification和multiclass classification的例子。在binary classification中输出只有两个分类选择，但是在multiclass classification中可以有多个分类选择。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/f46ccf37-7491-4e44-8ef9-0b9bec8c3ba1)
# softmax
## softmax原理
如图所示为softmax regression中对各种output的分析。softmax先计算一个中间变量z，再根据这个z计算对应的a。其中a1到a4分别表示输出结果为1到4的概率。可以看到在红框中的aj的定义公式，将每一种可能的output对应的概率相加结果为1。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bd16454f-3643-42bc-8a5d-79d299564faf)</br>
如图所示为softmax regression从对损失的定义，如果y=1，那么loss为-loga1，以此类推到an。</br>
图中的函数图像展示了对于aj举例的损失，因为a的计算结果表示一个概率，所以a的大小是从0到1的。如果y=j，那么当aj趋近于1时，损失就比较小；当aj趋近于0时，损失就非常大。在每一个训练的例子中，只能选择一个y。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/fbb50ed2-f93e-4d46-bacf-c5bb9b0de572)</br>
## softmax输出
如图所示为softmax输出的网络。其中output layer有10个unit，a3是一个10维的向量，每一个分量表示一个概率。可以看到softmax regression与以往的logistic regression最大的不同在于，softmax regression中的输出向量中的每一个分量，可以看成是layer3中计算的从z1到z10的函数，每一个分量的计算都需要从z1到z10的所有值。但是在logistic regression中，activation中的每一个分量其计算仅仅需要z中的一个一个分量，例如a1中的分量计算只需要z1中的一个分量。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/933b8b19-996c-4ece-9b91-c4b5a3ee7422)</br>
## softmax实现
如图所示为完整的softmax实现，这个实现的过程中改进了原始的softmax方法，使得计算的误差更小。关键的步骤在于编译的阶段中选择loss function中from_logits=True，这个选择使得在loss的计算过程中不再计算a作为中间变量，而是直接将a的表达式代入到loss的计算过程中。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/e39982b7-d375-4a25-8980-a219243fca00)</br>
如图所示在最后的output layer中，激活函数选择linear，这是因为在改进的softmax实现中，输出的不再是从a1到a10的概率值，而是z1到z10。这样，在predict的环节中，作为预测基础的logits就是从z1到z10。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/66375867-8909-4974-96cc-d00e8dc5ddff)











