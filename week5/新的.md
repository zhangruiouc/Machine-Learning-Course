# Tensorflow实现
图中所示的是一个Tensorflow网络的实现。网络的实现可以分为三部分：</br>
(1)利用dense创建每一层网络，同时用sequential将每一层网络连接。</br>
(2) 编译model，同时根据需要指定一个loss function。</br>
(3)利用fit来训练模型，epochs是梯度下降的递归次数。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bf69977a-5d3c-4d27-a1f9-62cf68fad6af)</br>
# 不同的激活函数
图中所示的是三种不同的activation函数。网络层中的每一层都可以选择一个激活函数。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/d6f0a383-75aa-4d69-9ee3-7e4dbf0d541b)</br>
三种激活函数分别是：</br>
(1)linear activation function，也可以理解为no activation function。函数值的范围可正可负。</br>
(2)sigmoid。函数值的范围从0到1。</br>
(3)ReLU。函数值大于等于0。</br>
# 如何选择激活函数
激活函数的选择对网络的性能有较大的影响。根据经验，在hidden layer中，往往选择ReLU作为激活函数。在output layer中选择激活函数可以按照如下的方法：</br>
(1)如果我们想解决的是一个binary classification问题，可以选择sigmoid。</br>
(2)如果我们想解决的问题中输出的是一个可正可负的数，选择linear activation。</br>
(3)如果我们想解决的问题的输出都是大于等于0的，那么我们可以选择ReLU。</br>
如图所示是一个简单的网络，在hidden layer中我们选择ReLU，而在output layer中我们选择sigmoid。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/92611eda-c576-4ae4-86df-9db60ffb28d0)</br>
需要注意的是，我们一般在hidden layer中不使用linear activation。举例说明，如果我们在每一层的hidden layer中都是用linear activation。那么我们的网络最后实现的功能仅仅与最后一层的output layer有关，如果我们的output layer选择sigmoid，那么整个网络所实现的功能仅仅只有logistic regression，前面那么多层的网络都在做无用功。</br>
# multiclass classification
如图所示为binary classification和multiclass classification的例子。在binary classification中输出只有两个分类选择，但是在multiclass classification中可以有多个分类选择。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/f46ccf37-7491-4e44-8ef9-0b9bec8c3ba1)
# Softmax与其改进







