# 支持向量机(Support Vector Machines)
## 优化目标
如图所示是用于逻辑回归的sigmoid函数，我们可以将逻辑回归的损失函数稍作变形可以开始建立支持向量机</br>
![3d12b07f13a976e916d0c707fd03153c](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/e15d28bb-7362-46c1-801c-416c14918eb7)</br>
如图所示为SVM中的损失函数，可以分为y=1和y=0的情况。</br>
![66facb7fa8eddc3a860e420588c981d5](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/545224de-fd32-4a48-be63-72ca23c83904)</br>
如图所示我们可以将逻辑回归中的损失函数里的项用cost0和cost1项来代替，同时提取一个负号就可以得到这个式子的前半部分。后半部分是正规化的的部分。可以看到，在SVM的损失函数中，我们去除了1/m这一项，这同样可以得到最优的Θ，因为1/m只是一个常量。</br>
![59541ab1fda4f92d6f1b508c8e29ab1c](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bb9aa3d5-c604-4de9-a563-b3b42701d832)</br>
在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们要做到两项的平衡。这里我们对逻辑回归中的损失函数表达式A+λB修改可以得到CA+B，其中C=1/λ。λ是正规化的参数，可以通过调节λ的大小解决high bias和high variance的问题。
## 大边界的直观理解
![cc66af7cbd88183efc07c8ddf09cbc73](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/cefa2330-edd2-4d27-a1aa-482e65e057c6)</br>
如图所示，当y=1时，我们希望z的值>=1，这样可以让我们的代价为0；当y=0的时候，我们希望z的值<=-1，这样可以让我们的代价最小。相对于逻辑回归，SVM可以理解为插入了额外的安全因子，我们需要的是比0大很多的数，比如说1；或者可以是比0小很多的数，比如说-1。</br>
![e68e6ca3275f433330a7981971eb4f16](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/4b772c11-3a07-4db5-90c7-932dc1618844)</br>
如图所示我们可以利用SVM画出分界线，其中黑色的线可以把正样本和负样本分开，并且效果是比较好的。当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，这也是SVM具有鲁棒性的原因，因为黑线可以用一个最大的间距来分离样本，这也是他被叫做**大间距分类器**的原因。
![b8fbe2f6ac48897cf40497a2d034c691](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/3036ea81-e269-4f2f-9a0d-7b6b6baacc44)</br>
如图，当我们的C设置的很大时，可能会因为某几个异常点让我们的分类效果变差，可能会得到图中所示的粉色这条线。但是如果我们的C设置的不是特别大，我们可以最终得到黑色这条分界线。</br>
这是因为C=1/λ，如果C很大，相当于λ很小，也就可能导致过拟合的问题。如果C很大，可能会导致底拟合的问题。</br>
## 核函数1
给定一个训练样本x，我们利用x的各个feature计算与我们预先选定的**地标**(**landmarks**)的近似程度来选取新的特征f1，f2，f3。图中的地标为l1，l2和l3。</br>
![b9acfc507a54f5ca13a3d50379972535](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c914958f-0894-4e9f-974d-2bb5281af7fa)</br>
图中所示的f1的计算过程展示了高斯核函数，如果一个训练样本x与地标l之间的距离很近的话，那么f1计算的结果就接近于1。反过来，如果一个训练样本x与地标l之间的距离非常远的话，那么f1计算的结果就近似于0。高斯核函数的计算结果就是衡量训练样本与地标之间的距离。</br>
可以看出，图中的横坐标为x1，纵坐标为x2的情况下，只有当训练样本x与地标l1重合时计算出来的f1才是最大值1，随着训练样本x的改变，f的值也会跟着改变。f的值的改变速率取决于σ的大小。</br>
可以看出，当样本接近于地标时，高斯核函数的计算结果接近0，而当样本远离地标时，高斯核函数的计算结果接近1。</br>
![3d8959d0d12fe9914dc827d5a074b564](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/3e94533f-6ada-4566-a06b-659e471304f1)</br>
如图所示的红色封闭线，意味着当f1和f2为1，f3为0时，如果预测函数的表达式>=0，那么这个封闭线里的预测值为1。</br>
在预测时，我们采用的特征不是训练样本本身的特征，而是通过核函数计算出的新特征f1,f2,f3。</br>
## 核函数2

参考视频: 12 - 5 - Kernels II (16 min).mkv

在上一节视频里，我们讨论了核函数这个想法，以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中，我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。

如何选择地标？

我们通常是根据训练集的数量选择地标的数量，即如果训练集中有$m$个样本，则我们选取$m$个地标，并且令:$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},.....,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：

![](../images/eca2571849cc36748c26c68708a7a5bd.png)

![](../images/ea31af620b0a0132fe494ebb4a362465.png)

下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：

• 给定$x$，计算新特征$f$，当$θ^Tf>=0$ 时，预测 $y=1$，否则反之。 

相应地修改代价函数为：$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $，

$min C\sum\limits_{i=1}^{m}{[{{y}^{(i)}}cos {{t}_{1}}}( {{\theta }^{T}}{{f}^{(i)}})+(1-{{y}^{(i)}})cos {{t}_{0}}( {{\theta }^{T}}{{f}^{(i)}})]+\frac{1}{2}\sum\limits_{j=1}^{n=m}{\theta _{j}^{2}}$
在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $时，我们用$θ^TMθ$代替$θ^Tθ$，其中$M$是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。

理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。

在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如**liblinear**,**libsvm**等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。

另外，支持向量机也可以不使用核函数，不使用核函数又称为**线性核函数**(**linear kernel**)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。

下面是支持向量机的两个参数$C$和$\sigma$的影响：

$C=1/\lambda$

$C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；

$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差；

$\sigma$较大时，可能会导致低方差，高偏差；

$\sigma$较小时，可能会导致低偏差，高方差。

如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。

### 12.6 使用支持向量机

参考视频: 12 - 6 - Using An SVM (21 min).mkv

目前为止，我们已经讨论了**SVM**比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用**SVM**。你实际上所需要的一些东西：支持向量机算法，提出了一个特别优化的问题。但是就如在之前的视频中我简单提到的，我真的不建议你自己写软件来求解参数${{\theta }}$，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决**SVM**最优化问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是**liblinear**和**libsvm**，但是真的有很多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。

在高斯核函数之外我们还有其他一些选择，如：

多项式核函数（**Polynomial Kerne**l）

字符串核函数（**String kernel**）

卡方核函数（ **chi-square kernel**）

直方图交集核函数（**histogram intersection kernel**）

等等...

这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer's定理，才能被支持向量机的优化软件正确处理。

多类分类问题

假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有$k$个类，则我们需要$k$个模型，以及$k$个参数向量${{\theta }}$。我们同样也可以训练$k$个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。

尽管你不去写你自己的**SVM**的优化软件，但是你也需要做几件事：

1、是提出参数$C$的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。

2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的**SVM**（支持向量机），这就意味这他使用了不带有核函数的**SVM**（支持向量机）。

从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？

**下面是一些普遍使用的准则：**

$n$为特征数，$m$为训练样本数。

(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。

(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。

(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。

值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。

今天的**SVM**包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归，或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是：逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核函数的**SVM**，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的**SVM**另一个也很有可能很有效。但是随着**SVM**的复杂度增加，当你使用不同的内核函数来学习复杂的非线性函数时，这个体系，你知道的，当你有多达1万（10,000）的样本时，也可能是5万（50,000），你的特征变量的数量这是相当大的。那是一个非常常见的体系，也许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需要逻辑回归的事情。

最后，神经网络使用于什么时候呢？ 对于所有的这些问题，对于所有的这些不同体系一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会使用神经网络的原因是：对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果你有一个非常好的**SVM**实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，**SVM**具有的优化问题，是一种凸优化问题。因此，好的**SVM**优化软件包总是会找到全局最小值，或者接近它的值。对于**SVM**你不需要担心局部最优。在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用**SVM**的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比**SVM**慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是**SVM**这方面更加重要。但是，已经说过了，**SVM**仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、**SVM**一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。（编者注：当时**GPU**计算比较慢，神经网络还不流行。）

机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具，你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。
