# 支持向量机(Support Vector Machines)
## 优化目标
如图所示是用于逻辑回归的sigmoid函数，我们可以将逻辑回归的损失函数稍作变形可以开始建立支持向量机</br>
![3d12b07f13a976e916d0c707fd03153c](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/e15d28bb-7362-46c1-801c-416c14918eb7)</br>
如图所示为SVM中的损失函数，可以分为y=1和y=0的情况。</br>
![66facb7fa8eddc3a860e420588c981d5](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/545224de-fd32-4a48-be63-72ca23c83904)</br>
如图所示我们可以将逻辑回归中的损失函数里的项用cost0和cost1项来代替，同时提取一个负号就可以得到这个式子的前半部分。后半部分是正规化的的部分。可以看到，在SVM的损失函数中，我们去除了1/m这一项，这同样可以得到最优的Θ，因为1/m只是一个常量。</br>
![59541ab1fda4f92d6f1b508c8e29ab1c](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bb9aa3d5-c604-4de9-a563-b3b42701d832)</br>
在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们要做到两项的平衡。这里我们对逻辑回归中的损失函数表达式A+λB修改可以得到CA+B，其中C=1/λ。λ是正规化的参数，可以通过调节λ的大小解决high bias和high variance的问题。
## 大边界的直观理解
![cc66af7cbd88183efc07c8ddf09cbc73](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/cefa2330-edd2-4d27-a1aa-482e65e057c6)</br>
如图所示，当y=1时，我们希望z的值>=1，这样可以让我们的代价为0；当y=0的时候，我们希望z的值<=-1，这样可以让我们的代价最小。相对于逻辑回归，SVM可以理解为插入了额外的安全因子，我们需要的是比0大很多的数，比如说1；或者可以是比0小很多的数，比如说-1。</br>
![e68e6ca3275f433330a7981971eb4f16](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/4b772c11-3a07-4db5-90c7-932dc1618844)</br>
如图所示我们可以利用SVM画出分界线，其中黑色的线可以把正样本和负样本分开，并且效果是比较好的。当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，这也是SVM具有鲁棒性的原因，因为黑线可以用一个最大的间距来分离样本，这也是他被叫做**大间距分类器**的原因。
![b8fbe2f6ac48897cf40497a2d034c691](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/3036ea81-e269-4f2f-9a0d-7b6b6baacc44)</br>
如图，当我们的C设置的很大时，可能会因为某几个异常点让我们的分类效果变差，可能会得到图中所示的粉色这条线。但是如果我们的C设置的不是特别大，我们可以最终得到黑色这条分界线。</br>
这是因为C=1/λ，如果C很大，相当于λ很小，也就可能导致过拟合的问题。如果C很大，可能会导致底拟合的问题。</br>
## 核函数1
给定一个训练样本x，我们利用x的各个feature计算与我们预先选定的**地标**(**landmarks**)的近似程度来选取新的特征f1，f2，f3。图中的地标为l1，l2和l3。</br>
![b9acfc507a54f5ca13a3d50379972535](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c914958f-0894-4e9f-974d-2bb5281af7fa)</br>
图中所示的f1的计算过程展示了高斯核函数，如果一个训练样本x与地标l之间的距离很近的话，那么f1计算的结果就接近于1。反过来，如果一个训练样本x与地标l之间的距离非常远的话，那么f1计算的结果就近似于0。高斯核函数的计算结果就是衡量训练样本与地标之间的距离。</br>
可以看出，图中的横坐标为x1，纵坐标为x2的情况下，只有当训练样本x与地标l1重合时计算出来的f1才是最大值1，随着训练样本x的改变，f的值也会跟着改变。f的值的改变速率取决于σ的大小。</br>
可以看出，当样本接近于地标时，高斯核函数的计算结果接近0，而当样本远离地标时，高斯核函数的计算结果接近1。</br>
![3d8959d0d12fe9914dc827d5a074b564](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/3e94533f-6ada-4566-a06b-659e471304f1)</br>
如图所示的红色封闭线，意味着当f1和f2为1，f3为0时，如果预测函数的表达式>=0，那么这个封闭线里的预测值为1。</br>
在预测时，我们采用的特征不是训练样本本身的特征，而是通过核函数计算出的新特征f1,f2,f3。</br>
## 核函数2
本小节主要解决的问题是如何选取合适的地标。</br>
我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个样本，则我们选取m个地标，并且令:</br>
l(1)=x(1),l(2)=x(2)...l(m)=x(m)。</br>
即让每一个地标一开始都设置为原来的训练样本，这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，如此一来当我们计算x(i)与l(i)之间的高斯核函数的计算结果时(即计算sim(x(i),l(i)))，其计算结果为1，如图所示。</br>
![eca2571849cc36748c26c68708a7a5bd](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/b517c1e3-fe08-479b-b08b-84f88bb1e391)</br>
![ea31af620b0a0132fe494ebb4a362465](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/bceea910-e0fd-4bf4-a740-252eec5d38b5)</br>
下面我们将核函数运用到支持向量机中:</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/552b9845-6a68-4bb7-b6b7-1b9ca32c0b3f)</br>
如图所示为计算f的过程，同时也包含训练的内容。在具体实施的过程中，还需要对正规项进行一些调整，计算Σ求和时从1到m来计算，这里可以简单的把n=m，这样做的目的是为了简化计算。</br>。
下面是支持向量机的两个参数C和σ的影响：</br>
C=1/λ
C较大时，相当于1/λ较小，可能会导致过拟合，高方差；
C较小时，相当于1/λ较大，可能会导致低拟合，高偏差；
σ较大时，可能会导致低方差，高偏差；
σ较小时，可能会导致低偏差，高方差。
## 使用支持向量机
今天我们已经拥有了优秀的**SVM**优化软件，但是我们也需要做几件事：
1、提出参数C的选择。
2、需要选择内核参数或想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。
从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？
**下面是一些普遍使用的准则：**
n为特征数，m为训练样本数。
(1)如果相较于m而言，n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。
(2)如果n较小，而且m大小中等，使用高斯核函数的支持向量机。
(3)如果n较小，而m较大，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。
