在本周的课程中，主要学习了神经网络中的各种概念，比如前向传播算法；什么是layer和unit，他们的功能是什么；tensorflow和numpy的数据形式，主要研究关于矩阵和向量的内容；矩阵的乘法如何应用到神经网络中。
# 神经网络中的layer和units
神经网络层的英语是neural network layer。我认为这个描述很有助于我对神经网络的理解。在本周的课程学习后，我把一个神经网络分为一个一个的网络层layer，网络层中可以有一个或多个的units，这些units可以执行一个个的logistic regression算法。每个unit计算得到的logistic regression都作为中间向量activation的一部分。通过这样的层层提取，我们可以在最后的layer中获得一个输出的output，这个output可以作为一个新的logistic regression的输入来作为判断。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/816113b0-ec7e-4b71-aa0d-834ed5527272)</br>
如图所示展示了一个简单的neural network layer。其中。layer0可以理解为输入层，传入一个三维的vector，layer1中的每一个圆都代表一个unit，分别有自己的w和b，通过logistic regression来计算a1向量的一部分内容。最后将a1整理可以得到，a1也是一个三维的vector，这个a1作为layer2的输入。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/e165c024-dd20-4d54-994b-4f66a767642d)</br>
如图所示layer2中只有一个unit，所以输出的a2是一个scalar。a2可以做一个二级制的预测，来判断最后的预测结果是1还是0。
# 神经网络中的前向传播 
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/ce42f8eb-bf01-4b17-8653-3ec243109bbc)</br>
如图所示是一个具有三层layer的神经网络。其中，a1到a3分别是layer1到layer3的activation vector。a3作为layer4的输入，可以通过logistic regression得到a4。图中的方框部分给出了activation vector的计算公式，涉及到的有本层的不同unit的w和b，以及上一层的activation vector。layer4的输出可以是一个概率，例如在识别手写数字0和1的项目中，可以把a4作为一个二进制分类的标签，来判断识别的数字是0还是1。
# tensorflow中的数据形式
tensorflow可以表示矩阵和向量，代码可以表示如下。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/ca6ba04f-eb41-4e94-acd0-fbe98840812a)</br>
如图所示，tensorflow中可以用两层中括号来表示矩阵，最外面的中括号的出现表示这是一个矩阵形的数据结构。内层的矩阵则表示矩阵中的元素，例如图中的1x2和2x1的两个矩阵。tensorflow还可以表示一位的向量，这种情况下只有一个中括号表示。 
# 神经网络的搭建
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/6c3b6af1-000b-43ee-8597-b847e9de1f76)</br>
如图所示是一个简单的单层神经网络的搭建。分别创建layer1和layer2，activation选择常见的sigmoid，利用sequential来将两个layer组合成一个model。x矩阵是一个4x2的矩阵，y是一个一维的向量(课程中把这种只有一行的向量理解成没有列的向量，记作一维向量，但是我比较习惯用向量中出现的数的多少来表示几维向量。这里沿用课程中的说法)。利用model.compile，model.fit和model.predict来分别实现编译，训练和预测的功能。
# 单个网络上的前向传播
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/2b682e59-f773-4d92-be57-287119f40716)</br>
如图所示给出了计算a1不同分量的计算公式，利用dot的点积可以计算出a1_1到a1_3，将a1_1到a1_3组合可以得到a2。
前向传播可以用如下的简单代码实现：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/d5397802-1f49-4100-811e-82b6015b13c3)</br>
如图所示，课程中定义了一个dense函数，该函数先计算出W矩阵的列数将其存到units中。在一个for循环中每一次都提取出W矩阵的一列，然后利用np.dot来计算向量点积，接着利用sigmoid的g(z)函数分别计算a_out的分量。</br>
接着在定义的sequential函数中，分别调用dense函数来计算a1到a4，最后将输出的a4作为最后的返回值。
# 矩阵的乘法实现
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/442921c7-cd53-44d5-872d-d08a1fdac13a)</br>
如图所示，AT矩阵是A矩阵的转置，W是一个2x3的矩阵，W矩阵的每一列都可以理解layer1中的一个unit对应的w向量。因为AT与W做矩阵的乘积后得到的是一个1x3的矩阵，所以b也是一个1x3的矩阵，这样可以将AT与W的乘积与b相加。在dense函数中，我们使用到matmul这个方法，可以直接实现矩阵的乘法而不仅仅是dot的点积功能。最后得到的a_out并返回，a_out也是一个1x3的矩阵，这里是a1，接着可以作为下一个layer2的输入。
# 实验
本周的实验用到了tensorflow和keras，因为各种package的版本兼容问题所以大部分时间花在环境配置上了，比如numpy的版本和tensorflow的兼容还有python3.11无法安装tensorflow需要降级到怕python3.7才能运行试验代码。
## Lab1 Neurons and Layers
本次实验中建立了一个简单的神经网络，并探索线性回归和逻辑回归之间的关系。</br>
由于线性回归和逻辑回归的实验内容与之前的实验类似，所以只展示出效果图和部分代码。</br>
### neurons without activation-linear model
如图所示，分别展示了在tensorflow和numpy两种方法下的预测结果，因为数据集中的点只有两个，处理起来也比较容易，可以看到tensorflow和numpy的处理效果都比较好。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c92d86a8-78be-49a0-ace9-f4ef8e9de490)</br>
### neurons with sigmoid activation
如图所示，activation选择sigmoid的logistic layer获得了和和numpy方法一样的效果，他们的输出值是一样的。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/a9dee2cb-9c63-4b5c-95a8-2a32285c722e)</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/22a3d28c-62d2-4381-96c4-98504568a094)</br>
## Lab2 CoffeeRoasting
数据集的打印结果如图所示：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/44958c77-e2b2-4442-a66c-4dbfafebdeb3)</br>
接下来利用keras的normalization layer处理相关工作，这里的代码不是很明白，在lab中显示下一周的课程会介绍。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/7d446855-dfec-4890-8778-78b7fc656883)</br>
本次的实验共有两个layer，不包括keras引入的normalization layer。输入层的vector有两个分量，所以在layer1中的W矩阵可以拆分成三列的w向量来理解；layer2中的W可以看成一个列向量，共有三个分量。layer1和layer2的W和b分别都在图中表示出来了。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/828322f1-e4d4-418b-b656-3ad43b2a5513)</br>
在fit数据后更新W和b的值如图所示：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/7517aacb-0053-4810-bed4-4869ab80246b)</br>
选择在训练中的W和b保存，lab中说不同的训练运行可能会产生不同的结果，我尝试了一下确实有可能产生变化。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/cb6a03b3-6e6b-4409-8dff-f9fabb3c179d)</br>
将输出的结果，其实是一个probability转换为一个1或者0的结果。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/1ece574a-5c95-408a-9831-11d2ea50e847)</br>
layer1中的每个unit分别能处理咖啡豆烘烤结果不好的三种情况。以unit0为例，处理的烘烤结果差的情况是低温情况。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/af82649a-6e03-4284-82cc-56ce4abe238e)</br>
最后一张图显示了整个网络的运行情况。分别表示原始的数据和经过预测后与决策阈值对比后得到的情况，可以看到是比较理想的。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/142d38e2-3398-4718-93dc-9a002ecec70f)



















