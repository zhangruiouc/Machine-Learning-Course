# 无监督学习
在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：</br>
![6709f5ca3cd2240d4e95dcc3d3e808d5](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c437b2a7-9fe3-46f9-8132-c90451d88ad1)</br>
在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，需要算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到的这些点集的算法，就被称为聚类算法。
## K-均值算法
**K-均值**是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。
**K-均值**是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:
首先选择K个随机的点，称为**聚类中心**（**cluster centroids**）；
对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
算法分为两个步骤，第一个**for**循环是赋值步骤，即：对于每一个样例i，计算其应该属于的类。第二个**for**循环是聚类中心的移动，即：对于每一个类K，重新计算该类的质心。
**K-均值**算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用**K-均值**算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。
![fed50a4e482cf3aae38afeb368141a97](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/963d722d-b1dc-48d7-bd27-b0fd4cd4d063)</br>
## 优化目标
K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此
K-均值的代价函数（又称**畸变函数** **Distortion function**）为：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/9b0a1d0a-5b4e-4d81-84da-9d61a1a92ad2)</br>
我们的的优化目标便是找出使得代价函数最小的参数，即：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/0b125a44-d2cc-4ae0-8932-0f126d7562b5)</br>
回顾刚才给出的:
**K-均值**迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。
## 随机初始化
在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点。以下是方法：</br>
1. 我们应该选择K<m，即聚类中心点的个数要小于所有训练集实例的数量
2. 随机选择K个训练实例，然后令K个聚类中心分别与这K个训练实例相等
![d4d2c3edbdd8915f4e9d254d2a47d9c7](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/83af814c-9876-4421-aa7a-0fb756486b2d)</br>
**K-均值**的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。
为了解决这个问题，我们通常需要多次运行**K-均值**算法，每一次都重新进行随机初始化，最后再比较多次运行**K-均值**的结果，选择代价函数最小的结果。这种方法在K较小的时候（2--10）还是可行的，但是如果K较大，这么做也可能不会有明显地改善。
## 选择聚类数
当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变K值，也就是聚类类别数目的总数。我们用一个聚类来运行**K均值**聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数J。K代表聚类数字。
![f3ddc6d751cab7aba7a6f8f44794e975](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c7f2ae53-debc-4439-8afa-3e68cf72b181)</br>
我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，从1到2，从2到3之后，会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，因此使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，K=3之后就下降得很慢。
# 降维
## 数据压缩
假设我们未知两个的特征：x_1:长度：用厘米表示；x_2：是用英寸表示同一物体的长度。这给了我们高度冗余表示，这两个基本的长度度量表示的是一个维度的东西，只有一个数测量这个长度。两者并没有什么不同。因此我们可以将数据从二维降至一维。
![2373072a74d97a9f606981ffaf1dd53b](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/faf50249-a0ed-4cef-b6f9-041bfd99d87e)</br>
我们还可以将数据从三维降到二维：</br>
![67e2a9d760300d33ac5e12ad2bd5523c](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/c26c1e9a-9874-4faa-8562-021ea149bbf2)</br>
## 数据可视化
在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。
![789d90327121d3391735087b9276db2a](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/2c98641c-e154-4b95-8788-1ddae90dbda9)</br>
假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如**GDP**，人均**GDP**，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。
![ec85b79482c868eddc06ba075465fbcf](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/b40991ae-8731-444b-bfe9-60c7d50e5f68)</br>
## 主成分分析问题
主成分分析(**PCA**)是最常见的降维算法。
在**PCA**中，我们要做的是找到一个方向向量（**Vector direction**），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。
![a93213474b35ce393320428996aeecd9](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/b656fd39-addb-4e43-ab3f-373a6715a99a)</br>
下面给出主成分分析问题的描述：
问题是要将n维数据降至k维，目标是找到向量使得总的投射误差最小。主成分分析与线性回顾的比较：
主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（**Projected Error**），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</br>
![7e1389918ab9358d1432d20ed20f8142](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/d977cc31-2c40-4cbd-b518-8bbb1af77d45)</br>
上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。
**PCA**将n个特征降维到k个，可以用来进行数据压缩。<br>
**PCA**技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。
**PCA**技术的一个很大的优点是，它是完全无参数限制的。在**PCA**的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。
## 主成分分析算法
**PCA** 减少n维到k维：
第一步是均值归一化。我们需要计算出所有特征的均值，然后令 x_j= x_j-μ_j。如果特征是在不同的数量级上，我们还需要将其除以标准差。
第二步是计算**协方差矩阵**（**covariance matrix**）Σ：
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/9b9fb09c-d853-4601-afa1-b223a69e2c83)</br>
第三步是计算协方差矩阵Σ的**特征向量**（**eigenvectors**）:
在 **Octave** 里我们可以利用**奇异值分解**（**singular value decomposition**）来求解，**[U, S, V]= svd(sigma)**。
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/eccb58b7-477d-4fac-b293-969aeb840194)</br>
对于一个 n×n维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。我们希望将数据从n维降至k维，只需要从U中选取前k个向量，获得一个n×k维度的矩阵，我们用U_reduce表示，然后通过如下计算获得要求的新特征向量z(i):
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/8c205991-c8d9-42c9-8c56-4e5e91b05e78)</br>
其中x是n×1维的，因此结果为k×1维度。
## 选择主成分的数量
主要成分分析是减少投射的平均均方误差：
训练集的方差为：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/7ba4f101-3014-495a-9d92-9f7f4f07dbd2)</br>
我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值。
我们可以先令k=1，然后进行主要成分分析，获得U_reduce和z，然后计算平均均方误差与训练集方差的比例是否小于1%。如果不是的话再令k=2，如此类推，直到找到可以使得比例小于1%的最小k值。
还有一些更好的方式来选择k，当我们在**Octave**中调用“**svd**”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。</r>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/0589f142-171d-40dd-b8f6-a3c238ccdf10)</br>
其中的S是一个n×n的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/5b23af6b-0639-4166-acfd-84988918fa3d)</br>
在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/5614b354-f5b7-4b45-a950-00c4e757bd9d)</br>
## 重建的压缩表示
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/429b8d03-a508-4963-b808-3184b9f12c44)</br>
如图所示为将压缩的数据重新表示的方法，这个方程可以做到将低维表示z回到未别压缩的表示。</br>
![image](https://github.com/zhangruiouc/Machine-Learning-Course/assets/130215873/3b970616-75bf-4c69-ace6-d594bcc74875)
我们可以将低维的表示z映射到近似的原有的高维的数据，可以将给定的10维的z(i)变回到原来的1000维的x(i)。
